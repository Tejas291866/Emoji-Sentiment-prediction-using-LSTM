# -*- coding: utf-8 -*-
"""Copy of Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L3GMj8tdQyzBXVSqaBK3CVRNi2lao202
"""

import numpy as np
import pandas as pd
import keras

train=pd.read_csv("/content/train_emoji.csv",header=None)
train.drop(labels=[2,3],axis=1,inplace=True)
train.head

!pip install emoji
import emoji

test=pd.read_csv("/content/test_emoji.csv",header=None)

test.head

emoji.get_emoji_unicode_dict('en')

emoji_dict={"0": ':beaming_face_with_smiling_eyes:',
"1":':1st_place_medal:',
"2":':face_with_crossed-out_eyes:',
"3":':face_with_thermometer:'}

for i in emoji_dict.values():
  print(emoji.emojize(i),end="  ")

"""pre processing text data

"""

x_train=train[0].values
x_train

y_train=train[1].values
y_train

"""embedding datat into glove(pre trained model)"""

f=open("/content/glove.6B.50d.txt",encoding='utf8')

embedding_matrix={}

for line in f:
  values = line.split()

  word=values[0]
  emb = np.array(values[1:],dtype='float')
  embedding_matrix[word]=emb

embedding_matrix

def get_embedding_matrix_for_data(data):
  maxLen=10
  embedding_data = np.zeros((len(data), maxLen, 50))
  for ix in range (data.shape[0]):
    words_in_sen = data[ix].split()
    for ij in range (len(words_in_sen)):
      if embedding_matrix.get(words_in_sen[ij].lower()) is not None:
        embedding_data[ix][ij] = embedding_matrix[words_in_sen[ij].lower()]

  return embedding_data

x_train = get_embedding_matrix_for_data(x_train)

x_train

from keras.utils.np_utils import to_categorical

y_train=to_categorical(y_train)
y_train

"""Model build"""

from keras.models import Sequential

from keras.layers import LSTM, SimpleRNN, Dense, Dropout

model=Sequential()
model.add(LSTM(units= 64, input_shape= (10,50),return_sequences=True))
model.add(Dropout(0.3))
model.add(LSTM(units= 32))
model.add(Dropout(0.2))
model.add(Dense(units=10,activation='relu'))
model.add(Dense(units=5,activation='softmax'))

model.summary()

model.compile(optimizer= 'adam',loss=keras.losses.categorical_crossentropy, metrics=['acc'],)

his =model.fit(x_train,y_train,validation_split= 0.1,batch_size=32,epochs=50)

model.evaluate(x_train,y_train)[1]

test[0]=test[0].apply(lambda x : x[:-1])

x_test=test[0].values
y_test=test[1].values

x_test

y_test

x_test = get_embedding_matrix_for_data(x_test)
y_test=to_categorical(y_test)

model.evaluate(x_test,y_test)[1]

temp_y_pred = model.predict(x_test)
y_pred = np.argmax(temp_y_pred,axis=1)

j=len(test)
for t in range(1,55):
  print(test[0].iloc[t])
  try:
    print("Predictions: ", emoji.emojize(emoji_dict[str(y_pred[t])]))
  except KeyError:
    print("Predictions: Key not found in emoji_dict")
  try:
    print("Actual: ", emoji.emojize(emoji_dict[str(test[1].iloc[t])]))
  except KeyError:
    print("Actual: Key not found in emoji_dict")
  print()